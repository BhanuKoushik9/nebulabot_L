{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pdf2image import convert_from_path, exceptions\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from gradio_client import Client, handle_file\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyAmGVxFhJYvzV07mBk4IbiYHbulwzN0Gb0\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Ollama client\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llava:34b\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def convert_pdf_to_images(pdf_file, output_folder):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_file)\n",
    "        image_paths = []\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = os.path.join(output_folder, f'slide_{i + 1}.png')\n",
    "            image.save(image_path, 'PNG')\n",
    "            image_paths.append(image_path)\n",
    "        return image_paths\n",
    "    except (exceptions.PDFInfoNotInstalledError, exceptions.PDFPageCountError, exceptions.PDFSyntaxError) as e:\n",
    "        print(f\"Error converting PDF to images: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_pdf(pdf_file, output_folder):\n",
    "    try:\n",
    "        image_paths = convert_pdf_to_images(pdf_file, output_folder) \n",
    "        print(image_paths)\n",
    "        print(f\"Extracted {len(image_paths)} images from {pdf_file} to {output_folder}\")     \n",
    "        return image_paths\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_file}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"attention_is_all_you_need.pdf\"\n",
    "filename = os.path.basename(file_path)\n",
    "output_folder = \"extracted_pngimages_fromPDF\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extracted_pngimages_fromPDF\\\\slide_1.png', 'extracted_pngimages_fromPDF\\\\slide_2.png', 'extracted_pngimages_fromPDF\\\\slide_3.png', 'extracted_pngimages_fromPDF\\\\slide_4.png', 'extracted_pngimages_fromPDF\\\\slide_5.png', 'extracted_pngimages_fromPDF\\\\slide_6.png', 'extracted_pngimages_fromPDF\\\\slide_7.png', 'extracted_pngimages_fromPDF\\\\slide_8.png', 'extracted_pngimages_fromPDF\\\\slide_9.png', 'extracted_pngimages_fromPDF\\\\slide_10.png', 'extracted_pngimages_fromPDF\\\\slide_11.png', 'extracted_pngimages_fromPDF\\\\slide_12.png', 'extracted_pngimages_fromPDF\\\\slide_13.png', 'extracted_pngimages_fromPDF\\\\slide_14.png', 'extracted_pngimages_fromPDF\\\\slide_15.png']\n",
      "Extracted 15 images from attention_is_all_you_need.pdf to extracted_pngimages_fromPDF\n"
     ]
    }
   ],
   "source": [
    "if file_path.lower().endswith('.pdf'):\n",
    "    image_files = process_pdf(file_path, output_folder)\n",
    "else:\n",
    "    raise ValueError(\"PDF please.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_ocr_with_llm(ocr_text):\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"ocr_text\"],\n",
    "    template=\"\"\"\n",
    "    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\n",
    "    OCR Text:\n",
    "    {ocr_text}\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(prompt)\n",
    "    \n",
    "    # llm = Ollama(base_url='http://localhost:11434', model=\"llava:34b\", temperature=0.0)\n",
    "    llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", groq_api_key=\"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\" )\n",
    "\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()     \n",
    "    structured_ocr = chain.invoke(ocr_text)\n",
    "    \n",
    "    \n",
    "    print(\"Structured text from OCR:\", structured_ocr)\n",
    "    return structured_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://gokaygokay-florence-2.hf.space âœ”\n",
      "{'<OCR>': '\\nProvided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Attention Is All You NeedAshish Vaswani\\'Noam Shawezer\\'Nikki Parmar\\'Jakob Uszkoreit\\'Google BrainGoogle Researchavaswan@google.comnoam@Google.comnikipogogle.comGoogle Researchuszgogle.comLion Jones\\'Aidan N. Comer\\'Lukasz Kaiser\\'Google ResearchUniversity of TorontoGoogle Brain11lion@google.comaidananda, toronto.edulukaszkaiser@google_comIllia Poloshkin\\'11lila.poloshkin@gmail.comAbstractThe dominant sequence translation models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperformational neural networks are a new network architecture, the Transformermechanism, and the idea. A new network structure, the Transformormechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutionalsbe superior to evaluate while being more parallelable and requiring significantlyless time to train. While other models 2014 BLEU in the WTMT 2014 English-ensemble, by over 2 BLEu, the WNT 2014 English to the first translation task,the model establishes a single-language state-of-the-art bleue bleue of 41.8 afterbest models for a literature. Show how the Transferner generalizes both it toother tasks by applying it successfully to English Constituency parsing both witharXiv.1706.0376027 | [cs.CLI] | 2 Aug 2023\"Equal contribution. Listing is random, Jakob proposed replacing RNNS with self-transformer models andthe effort to evaluate this article. Ashish, Ashish and Jakob developed and implemented the transferner models andattention, the parameter-free position tuned and became the other persons involved in nearly everydetail. Niki designed, implemented, and evaluated countless models in our original codebe and implemented. Nikis developed, implemented and evaluated the model person involved in nearby everyefficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of andour research.Work performed while at Google Brain.1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**Title:** Attention Is All You Need\n",
      "\n",
      "**Authors:**\n",
      "\n",
      "1. Ashish Vaswani\n",
      "2. Noam Shawezer\n",
      "3. Nikki Parmar\n",
      "4. Jakob Uszkoreit\n",
      "5. Lion Jones\n",
      "6. Aidan N. Comer\n",
      "7. Lukasz Kaiser\n",
      "\n",
      "**Affiliations:**\n",
      "\n",
      "1. Google Brain\n",
      "2. Google Research\n",
      "3. University of Toronto\n",
      "\n",
      "**Contact Information:**\n",
      "\n",
      "1. Ashish Vaswani: avaswan@google.com\n",
      "2. Noam Shawezer: noam@Google.com\n",
      "3. Nikki Parmar: nikipogogle.com\n",
      "4. Jakob Uszkoreit: uszgogle.com\n",
      "5. Lion Jones: lion@google.com\n",
      "6. Aidan N. Comer: aidananda, toronto.edu\n",
      "7. Lukasz Kaiser: lukaszkaiser@google.com\n",
      "8. Illia Poloshkin: lila.poloshkin@gmail.com\n",
      "\n",
      "**Abstract:**\n",
      "\n",
      "The dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\n",
      "\n",
      "**Paper Information:**\n",
      "\n",
      "* Paper ID: arXiv.1706.03760\n",
      "* Category: cs.CLI\n",
      "* Date: 2 Aug 2023\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* Equal contribution\n",
      "* Listing is random\n",
      "* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\n",
      "* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\n",
      "* Niki designed, implemented, and evaluated countless models in our original code and implemented.\n",
      "* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\n",
      "* Work performed while at Google Brain.\n",
      "* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.']\n",
      "{'<OCR>': '\\n1 IntroductionRecurrent neural networks, long-short-term memory (13 and gated recurrent 7) neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andin particular have been primarily established as time of recurrent language models and encoder-decoderefforts have since been continued to push the boundaries of the current language models in the input and outputarchitectures (E3, E2, E3, and E3).Recurrent models typically factor computation along the symbol positions of the input & outputstates have been the previous state of computation time, which generate a sequence of hiddensequential prerequisites parallelization within training times, which becomes critical at longersignificant nature in computations efficiently through fractionation tricks. This conditionalcomputation has also been improving model performance in case of the latter. 2) and the fundamentalcomputations have also been an integral part of completing sequence modeling (and tranduc-tion models in various tasks, allowing modeling of dependencies without such direction and distance inthe input or output tasks. All in all of these cases (2) however, such such mechanismsewing entirely on an attention mechanism in global global dependencies between new and output.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadThe Transformer allows for significantly more parallelization to reach a reach of the Art intransformation quality after being aligned for more little hours on eight P100 GPUs.2 BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[1g), Bythene(18) and Conv(25) is (9) all of which use computational neural networks in basic buildingblock, complex hidden representations in parallel use and input output positions. In these models,the number of operations required to relate signals from two arbitrary input positions growsreduced to a certain number of operations, effect at the cost of reduced Multi-Resolution this isit more difficult to learn dependencies between two columns (1) and Transformer this isdescribed in a section numbered postpointed, an effort to counteract the effect of Multi-Head Attention dueSelf-attention, sometimes called called compute is an attempt mechanism self-different positionsof a single sequence in order to compute a representation of the sequence. Self-different position has beentextual maintenance and learning task-dependent sentence representations (27) and 27) instead of sequence-End-to-end memory networks are based on a recent sequence mechanism question answering andaligned recurrence in learning tasks. However, a Transformer is the first transition model relyingto be the best of our knowledge, however, to perform on simple language question answering-entirely on self-attENTION to compute representations of input and input without using sequence-aligned RNNs and convolution. In the following sections, we will describe the Without Motivateself-attenuation and discuss its advantages over models such as [1] and [2]3 Model ArchitectureMost\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Recurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\n",
      "\n",
      "**Background**\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\n",
      "\n",
      "**Transformer**\n",
      "\n",
      "The Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\n",
      "\n",
      "**Model Architecture**\n",
      "\n",
      "The Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\n",
      "\n",
      "**Self-Attention**\n",
      "\n",
      "Self-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\n",
      "\n",
      "**Advantages**\n",
      "\n",
      "The Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].']\n",
      "{'<OCR>': '\\nOutputProbabilitiesSottmaxLinearAdd & NormFeedFeedAdd & NormalAttentionFeedMulti-HeadForwardNxNxAdd + NormAdd + NormalMulti-HeadAttentionAttentionPositionalPositionalEncodingEncodingInputOutputEncodingEmbedding(shifted right)InputsOutputsFigure 1: The Transformer - model-attention and point-wise, fullyConnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1respectively.3.1 Encoder and Decoder StacksEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has twosub-layers. The first is a multi-decoder self-attenuation mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection (1) around each ofthe two layers. The second layer is a sub-layer, which is similar to the other two layers in the sub-layeredlayers, produce outputs of dimension (5:1). The function implemented as well as the embeddingiself. To facilitate these residual connections, 5:1-2 sub-lays are a 6-dimensional layer, in addition to the twoDecoder: In each decoder layer, the composer of A stack of 3 identical layers, which performs multi-headsub-layer in each encoder stack, followed by layer normalization. We also modify the self-attentionsaround layer in the decoder stack to prevent embeddings by one of these sub-sections. Thismask-layer is in the deoder stack and prevents embedding is off of the right and left halves that thepredictions for position can depend only on the known outputs at positions less than 1.2 AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sum3\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**The Transformer Model**\n",
      "\n",
      "**Model Architecture**\n",
      "\n",
      "The Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\n",
      "\n",
      "**Encoder**\n",
      "\n",
      "* The encoder is composed of a stack of 6 identical layers.\n",
      "* Each layer has two sub-layers:\n",
      "\t1. A multi-decoder self-attenuation mechanism.\n",
      "\t2. A simple, position-wise fully connected feed-forward network.\n",
      "* Residual connections are employed around each of the two layers.\n",
      "\n",
      "**Decoder**\n",
      "\n",
      "* The decoder is composed of a stack of 3 identical layers.\n",
      "* Each layer performs multi-head self-attentions, followed by layer normalization.\n",
      "* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\n",
      "\n",
      "**Attention**\n",
      "\n",
      "* An attention function maps a query and a set of key-value pairs to an output.\n",
      "* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\n",
      "\n",
      "**Additional Details**\n",
      "\n",
      "* The embedding is implemented as well as the function.\n",
      "* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\n",
      "* The model uses a residual connection (1) around each of the two layers.\n",
      "\n",
      "**Figure 1**\n",
      "\n",
      "The Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.']\n",
      "{'<OCR>': '\\nSealed Dot-Product AttentionMulti-Head AttentionLinearLinearSoftMaxConcelMask (opt)Scaled Dot-ProducthScaleAttentionLinearScaleLinearKKFigure 2: (left) Sealed Dot.Product Attention, (right) Multi-Head Attention consists of severalattention layers running in parallel.of the values, where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key.3.2.1 Sealed Dat-Product AttentionWe call our particular attention \"Scaled Do-Product Action\" (Figure 3). The input consists ofqueries and keys of dimension d, d, and values of dimension da. We compute the dot products of thequeries with all keys, divide each d, v, and apply a softmax function simultaneously, packed togetherquery. In practice, we compute the attention function of a set of queries that obtain the weights on thevalue of the matrix Q, the keys and values are also packed together to matrices K and V. We computethe matrix of outputs as:Attention(Q, K) = softmax(QK)The two most commonly used attention functions are additive addition 2) and dot-product multi-plicative attention. Dot-product attention is located in our algorithm, except for the scaling factorof a \"x\", Additive compute the compatibility function using a feed-forward network witha \"x\" and \"x\". Additive more space-efficient in practice, since it can be implemented highly optimizedmatrix multiplex. While for values of code, the two mechanisms perform similarly, additive attention outperformedd, the dot product grows large in magnitude, pushing the softmax functions into rows of d, we suspect that for large quantities ofextremally small gradients. To count this effect, we scale the dot projects by region3.22 Multi-Header AttentionInstead of performing a single attention function with d-model-dimensional keys, values and queries,we found it beneficial to linearly identify the queries, keys and value 1 times with different, learnedlinear projects to de, d/e, and d/f, and dimensions, respectively. On each project these versions of queries. Keys and values get perform that attention function in parallel, yielding d-dimensional\"To illustrate the product does not perform as the components of a k, k, and k, has an independent randomvariables with mean 0 and variance 1. Then their dot product, q, x, qk, q k, xk, k has mean0 and variance d.4\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**Title:** Sealed Dot-Product Attention and Multi-Head Attention\n",
      "\n",
      "**Section 3.2:** Attention Mechanisms\n",
      "\n",
      "**3.2.1 Sealed Dot-Product Attention:**\n",
      "\n",
      "* Input: queries and keys of dimension d, d, and values of dimension da\n",
      "* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\n",
      "* Formula: Attention(Q, K) = softmax(QK)\n",
      "* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\n",
      "* Dot-product attention is used in our algorithm, except for the scaling factor\n",
      "* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\n",
      "\n",
      "**3.2.2 Multi-Head Attention:**\n",
      "\n",
      "* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\n",
      "* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\n",
      "* Perform attention function in parallel on each projection, yielding d-dimensional output\n",
      "* This approach is beneficial and outperforms single-head attention\n",
      "\n",
      "**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\n",
      "\n",
      "**Figure 3:** Scaled Dot-Product Attention\n",
      "\n",
      "**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.']\n",
      "{'<OCR>': \"\\noutput values. These are concentrated and once again projected, resulting in the final values, asdepicted in Figure 2Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions. With a single attention head, averaging inhibits this.Multi-Head(Q,K,V) = Concat(head), (head),JWOwhere head), = Attention(QW), K,WK, WV,WV)Where the projections are parameter matrices W2 = R R R E R E E R R WK, K, WY, W Y, W V,WY,and I/O = (d) = (h) = [h) + (h). In this work we employ to = 8 parallel attention layers, or heads. For each of these we used, = d, = [d] = [b] = 'h). Due to the parallel dimension of each head, the total computational costis similar to single-head attachment with dimensionality.3.2.3 Applications of Attention in our ModelThe Transformer uses multi-attention layer, three different ways:. In encoder-multi attention layer, there comes from the previous decoder layer,and the memory keys and values come from the output in the encoder. This allows everyposition in the decoder to attend all positions in the input sequence. This mimics theposition of decoder-decoder attention mechanisms in sequence-sequence models such as. The encoder contains attention layers. In a self-attener layer all of the keys, valuesand queries from the same place, in case, the output of the previous layer in theencoder. Each position in the enoder can add to all positions to the previous layer of theenoder. Similarly, self-attenuation layers in the DECoder allow each position to attend toall self-assetment layer, in which the input position. We need to prevent leftoverinside of scaled data-product response by masking (setting) to all values in thisof the software which correspond to illegal connections. See Figure 3.3 Position to Atten-Feed-Forward NetworksIn addition to-atten sub-sub-sayers, each of the layers in our encoder and decoder contains a fullyconnected two linear network, which is applied to each position separately and identically. Thisconsists of two linear transformations with a applied ACTU activation in between.FFN(z)=max(z) =(1,2) +(h)w2+b)2)While the linear transformations are the same different positions, use different parametersfrom layer to layer. Another way of describing this is as the convolutionations with kernel size I.The dimensionality of output and output is (6,12, and the inner-layer has dimensionalitydff = 2014.34 Embedings and SoftmaxSimilarly to\\n\"}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Multi-Head Attention**\n",
      "\n",
      "* Allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "* With a single attention head, averaging inhibits this.\n",
      "\n",
      "**Mathematical Representation**\n",
      "\n",
      "* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\n",
      "* headj = Attention(QWj, K, WVj)\n",
      "* Where:\n",
      "\t+ Q, K, V are input matrices\n",
      "\t+ Wj are parameter matrices\n",
      "\t+ d is the dimensionality of the input\n",
      "\t+ h is the number of heads\n",
      "\t+ J is the number of parallel attention layers\n",
      "\n",
      "**Parameter Matrices**\n",
      "\n",
      "* W2 = [R, R, R, E, R, E, E, R]\n",
      "* WK, K, WY, WV are parameter matrices\n",
      "* I/O = (d) = (h) = [h] + [h]\n",
      "\n",
      "**Applications of Attention in our Model**\n",
      "\n",
      "* The Transformer uses multi-attention layers in three different ways:\n",
      "\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\n",
      "\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\n",
      "\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\n",
      "\n",
      "**Preventing Leftover Inside Scaled Data-Product Response**\n",
      "\n",
      "* Masking (setting) all values in the software that correspond to illegal connections.\n",
      "\n",
      "**Position to Attention Feed-Forward Networks**\n",
      "\n",
      "* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\n",
      "* The network consists of two linear transformations with an applied ACTU activation in between.\n",
      "* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\n",
      "\n",
      "**Embeddings and Softmax**\n",
      "\n",
      "* Similar to [insert reference or description]\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]']\n",
      "{'<OCR>': '\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operationsfor different layer types, n = the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r = the size of the neighborhood in restricted self-attention.Layer TypeComplexity per LayerSequentialMaximum Path LengthSelf-AttentionOperations(0(1)(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35)(36)(37)(38)(39)(40)(41)(42)(43)(44)(45)(46)(47)(48)(50)(51)(52)(53)(54)(55)(56)(57)(58)(59)(60)(61)(62)(63)(64)(65)(66)(67)(68)(69)(70)(71)(80)(81)(82)(83)(84)(85)(86)(87)(88)(89)(90)(91)(92)(93)(94)(95)(96)(97)(98)(99)(100)(102)(104)(105)(106)(107)(108)(110)(111)(112)(113)(114)(115)(116)(117)(118)(119)(122)(123)(124)(125)(126)(127)(128)(129)(130)(131)(132)(133)(134)(135)(136)(137)(138)(139)(140)(145)(150)(176)(170)(171)(172)(173)(174)(175)(176)172)(176(187)(188)(189)(190)(191)(200)(220)(225)(226)(227)(270)(260)(267)(268)(269)(208)(209)(210)(211)(202)(223)(204)(205)(206)(207)(208)202)(207)202(208)(203)(214)(229)(206)203)(204)203(205)(207(208)203)204(206)204)(206(207)204)205(206)(208(209)(209)206(209)207(202)(202)202)203 (204)(203)203/204)(204(205)206)207 (206)208(207)(202(203)(203(204)206)(204/206)209(204)(207/208)209 (206)(203/208)(205)208)(207 (208)208 (209)209)202 (204)202/206(203)206 (207)208/208(205/206)(206/207)206/208/209(206/209)208)205/207(206(208).209(207(209).202(206).202)206.202(207).202)(\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\n",
      "\n",
      "| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\n",
      "| --- | --- | --- | --- |\n",
      "| 0 | 1 | 1 | 2 |\n",
      "| 1 | 1 | 2 | 3 |\n",
      "| 2 | 1 | 3 | 4 |\n",
      "| 3 | 1 | 4 | 5 |\n",
      "| 4 | 1 | 5 | 6 |\n",
      "| 5 | 1 | 6 | 7 |\n",
      "| 6 | 1 | 7 | 8 |\n",
      "| 7 | 1 | 8 | 9 |\n",
      "| 8 | 1 | 9 | 10 |\n",
      "| 9 | 1 | 10 | 11 |\n",
      "| 10 | 1 | 11 | 12 |\n",
      "| 11 | 1 | 12 | 13 |\n",
      "| 12 | 1 | 13 | 14 |\n",
      "| 13 | 1 | 14 | 15 |\n",
      "| 14 | 1 | 15 | 16 |\n",
      "| 15 | 1 | 16 | 17 |\n",
      "| 16 | 1 | 17 | 18 |\n",
      "| 17 | 1 | 18 | 19 |\n",
      "| 18 | 1 | 19 | 20 |\n",
      "| 19 | 1 | 20 | 21 |\n",
      "| 20 | 1 | 21 | 22 |\n",
      "| 21 | 1 | 22 | 23 |\n",
      "| 22 | 1 | 23 | 24 |\n",
      "| 23 | 1 | 24 | 25 |\n",
      "| 24 | 1 | 25 | 26 |\n",
      "| 25 | 1 | 26 | 27 |\n",
      "| 26 | 1 | 27 | 28 |\n",
      "| 27 | 1 | 28 | 29 |\n",
      "| 28 | 1 | 29 | 30 |\n",
      "| 29 | 1 | 30 | 31 |\n",
      "| 30 | 1 | 31 | 32 |\n",
      "| 31 | 1 | 32 | 33 |\n",
      "| 32 | 1 | 33 | 34 |\n",
      "| 33 | 1 | 34 | 35 |\n",
      "| 34 | 1 | 35 | 36 |\n",
      "| 35 | 1 | 36 | 37 |\n",
      "| 36 | 1 | 37 | 38 |\n",
      "| 37 | 1 | 38 | 39 |\n",
      "| 38 | 1 | 39 | 40 |\n",
      "| 39 | 1 | 40 | 41 |\n",
      "| 40 | 1 | 41 | 42 |\n",
      "| 41 | 1 | 42 | 43 |\n",
      "| 42 | 1 | 43 | 44 |\n",
      "| 43 | 1 | 44 | 45 |\n",
      "| 44 | 1 | 45 | 46 |\n",
      "| 45 | 1 | 46 | 47 |\n",
      "| 46 | 1 | 47 | 48 |\n",
      "| 47 | 1 | 48 | 50 |\n",
      "| 48 | 1 | 51 | 52 |\n",
      "| 52 | 1 | 53 | 54 |\n",
      "| 54 | 1 | 55 | 56 |\n",
      "| 56 | 1 | 57 | 58 |\n",
      "| 58 | 1 | 59 | 60 |\n",
      "| 60 | 1 | 61 | 62 |\n",
      "| 62 | 1 | 63 | 64 |\n",
      "| 64 | 1 | 65 | 66 |\n",
      "| 66 | 1 | 67 | 68 |\n",
      "| 68 | 1 | 69 | 70 |\n",
      "| 70 | 1 | 71 | 80 |\n",
      "| 80 | 1 | 81 | 82 |\n",
      "| 82 | 1 | 83 | 84 |\n",
      "| 84 | 1 | 85 | 86 |\n",
      "| 86 | 1 | 87 | 88 |\n",
      "| 88 | 1 | 89 | 90 |\n",
      "| 90 | 1 | 91 | 92 |\n",
      "| 92 | 1 | 93 | 94 |\n",
      "| 94 | 1 | 95 | 96 |\n",
      "| 96 | 1 | 97 | 98 |\n",
      "| 98 | 1 | 99 | 100 |\n",
      "| 100 | 1 | 102 | 104 |\n",
      "| 104 | 1 | 105 | 106 |\n",
      "| 106 | 1 | 107 | 108 |\n",
      "| 108 | 1 | 110 | 111 |\n",
      "| 111 | 1 | 112 | 113 |\n",
      "| 113 | 1 | 114 | 115 |\n",
      "| 115 | 1 | 116 | 117 |\n",
      "| 117 | 1 | 118 | 119 |\n",
      "| 119 | 1 | 122 | 123 |\n",
      "| 123 | 1 | 124 | 125 |\n",
      "| 125 | 1 | 126 | 127 |\n",
      "| 127 | 1 | 128 | 129 |\n",
      "| 129 | 1 | 130 | 131 |\n",
      "| 131 | 1 | 132 | 133 |\n",
      "| 133 | 1 | 134 | 135 |\n",
      "| 135 | 1 | 136 | 137 |\n",
      "| 137 | 1 | 138 | 139 |\n",
      "| 139 | 1 | 140 | 145 |\n",
      "| 145 | 1 | 150 | 176 |\n",
      "| 176 | 1 | 170 | 171 |\n",
      "| 171 | 1 | 172 | 173 |\n",
      "| 173 | 1 | 174 | 175 |\n",
      "| 175 | 1 | 176 | 172 |\n",
      "| 172 | 1 | 176 | 187 |\n",
      "| 187 | 1 | 188 | 189 |\n",
      "| 189 | 1 | 190 | 191 |\n",
      "| 191 | 1 | 200 | 220 |\n",
      "| 220 | 1 | 225 | 226 |\n",
      "| 226 | 1 | 227 | 270 |\n",
      "| 270 | 1 | 260 | 267 |\n",
      "| 267 | 1 | 268 | 269 |\n",
      "| 269 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 210 | 211 |\n",
      "| 211 | 1 | 202 | 223 |\n",
      "| 223 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 202 |\n",
      "| 202 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 203 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 208 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 207 | 208 |\n",
      "| 208 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 205 | 206 |\n",
      "| 206 | 1 | 208 | 209 |\n",
      "| 209 | 1 | 206 | 207 |\n",
      "| 207 | 1 | 202 | 203 |\n",
      "| 203 | 1 | 204 | 205 |\n",
      "| 205 | 1 | 203 | 204 |\n",
      "| 204 | 1 | 206 | 207 |\n",
      "| 207 | 1 |\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |']\n",
      "{'<OCR>': '\\nlength 7 is smaller than the representation dimensionality d, which is most often the case withsentence representations used by state-of-the-art models in machine translations, such as word-piece(8) and byte-pair (1) represent to improve computational performance for tasks involvingvery long sequences. Centered around the respective output position. This would increase size of inthe path length of O(n). We plan to investigate this approach further in future work.A single convolutional layer with kernel width is (8) does not connect two pairs of input and outputbetween any two in the case of diluted convolutionals (18) increasing the length of the longest pathsor O(0.0%) in two positions in the network. Convolutional layers are generally more expensive thanconsiderably, in (0.n). - even if a n, however, the complexity of a separateconvolutional is equal to the combination of self-attention layer and a point-wise feed-forward layer.As side bench, self-esteem would yield more interpretable models. We inspect attention distributionsfrom our models and present and discuss examples in the appendix. Not only any individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5 TrainingThis section describes the training regime for our models.5.1 Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs. Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37000 tokens for English-French, and split into significantly larger WMT2014 English French dialect tokens for 36 English-English tokens in a 3,000 word-piecebatch contained a set of sentence parts containing approximately 2500 source tokens. Each fragment has a 25000target tokens.5 2 Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models usingthe hyperperipheral described paper, 12 hours, for our step models (4,5 seconds. Wetrained the base models a total of 100,000 steps, or 12 hours. Our big model(decorated on thebottom line of table B) step time 1.0 seconds. The big models were trained for 300,000 step5.3 OptimizerWe used the Adam optimizer (20) with a 0.9,2 = 0.98 and = 10.9 - We varied the learningrate over the course of training, according to the formula.3)rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9,num = warmup-steps-1.5)This corresponds to increasing the learning rate linearly for the first warmup, steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\n",
      "* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\n",
      "\n",
      "**Approach**\n",
      "\n",
      "* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\n",
      "* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\n",
      "* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\n",
      "\n",
      "**Motivation**\n",
      "\n",
      "* Self-attention would yield more interpretable models.\n",
      "* We inspect attention distributions from our models and present and discuss examples in the appendix.\n",
      "* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "**Training**\n",
      "\n",
      "* **Training Data and Batching**\n",
      "\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\n",
      "\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\n",
      "* **Hardware and Schedule**\n",
      "\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\n",
      "\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\n",
      "\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\n",
      "* **Optimizer**\n",
      "\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\n",
      "\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.']\n",
      "{'<OCR>': '\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPS)ModelEN-DE EN-FREN-DEFEN-FRModel (18)Deep-Att + Post-Unk (39)24.7539.922.3-1019 1.4-1020GNMT+ RL (58)25.1640.962.6-1018.5-1029.5 -1020Consultant + Post/Unk Ensemble (59)26.3040.462.8-1010GNMT + RL (858)26,3031.1617.8 -1010Transformer (base model)27.634.1618.7-1011Transformer(base model).28.1128.3 -1019Residual Dropout We apply dropout (33) to the output of each sub-layer, before it is added to thesub-layer and normalized, in addition, we apply dropouts of the sum of the embeddings and thepositional encodings in both the encoder and decoder stacks. For the base model, we use a rate ofProp = 0.1Label Smoothing. During training, we employed label smoothing of value (2,0) Thishurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.6 Results6.1 Machine TranslationOn the WNT 2014 English-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best-previously reported scores (including assemblies) by more than 2.0BLEU, establishing a new state-to -BLEU model of 254. The configuration of this model isthe competitive model.Surpasses all previously published models and tasks, as a factor of a training cost of 4.0, ofOn the WT 2014 English translation tasks, our big transformer achieves a BLEUE score of 41.0.The WNT-2014 English-French translation task is our biggest point achieved by 1/4 in the training of theprevious rate of the model. Transformer models (model less than 1/1) for English-of French useddropout rate Paro = 0,1, instead of 0.3.For the base models, a single model obtained by averaging the last 5 checkpoints, whichwere chosen after experimentation on the development set, we averaged the last 20 checkpoints. Weused search for a beam size of 4 and length penalty set. We met 0.63 (0.63). These parameterswere chosen from the implementation of the deployment set, We met the maximum output duringTable 2 summarizes the results and compares our translation quality and\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\n",
      "\n",
      "| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\n",
      "| --- | --- | --- | --- |\n",
      "| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\n",
      "| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\n",
      "| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\n",
      "| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\n",
      "| Transformer (base model) | 27.63 | 34.16 | 1011 |\n",
      "| Transformer (base model) | 28.11 | 28.3 | 1019 |\n",
      "\n",
      "**Residual Dropout**\n",
      "\n",
      "* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\n",
      "* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\n",
      "* For the base model, use a rate of Prop = 0.1.\n",
      "\n",
      "**Label Smoothing**\n",
      "\n",
      "* Employ label smoothing of value (2,0) during training.\n",
      "* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "**Results**\n",
      "\n",
      "* **Machine Translation**\n",
      "\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\n",
      "\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\n",
      "\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\n",
      "\n",
      "**Model Configuration**\n",
      "\n",
      "* The competitive model is the big transformer model.\n",
      "* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\n",
      "* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\n",
      "* We used search for a beam size of 4 and length penalty set to 0.63.\n",
      "* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.']\n",
      "{'<OCR>': '\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the basemodel. All metrics are on the English-to-German translation development set, newsletter2013. Listedper-word per-word perplexities.per-words are per-wordspace, according to our byte-pair encoding, and should be compared toN modeldtr dth dth DthPapotrainPPL BLEUparamsbase6 512 2048 864646401.1 100K(dev)(dev)x10*65(A)512 2048851251251252.0k5.0K25.4(B)5.0125.1(5.1)25.2(C)256323225.5(4.825.0)1024102412812825.724.0(D)0.0.5.725.3(E)positional embedding instead of simiocids0.2.4.5-4.925.6big1024 40961603.300K4.3.3256.723development set, newest2013. We used beam search as described in the previous section, but nocheckpoint averaging. We present results results in Table 1.In Table B (A) we are the number of attention heads in the attention key and value dimensions,in Table B(B), we observe that the setting quality keys off with many single-headattention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2). Thissuggests that determining compatibility is not easy and over-sophisticated quality. Thisbigger models are better and dropout is very helpful in avoiding fit-fitting. (E) we replace oursinusoidal base encoding with learned positional embeddings (D) and observe nearly identicalresults to the base model.6.3 English Constituency ParsingTo evaluate if the Transferner can generalize to other tasks we performed experiments on Englishconstituency parsing. This task presents specific challenges. The output is subject to strong structuralconstituity parsing, and it is significantly longer than the Furtherment, RNN (RNN) sequenceconstraints and is significantly lower than the 1024 in the Wall Street Journal (WSI) portion of thePenn Treehouse (25), a 4-layer transformer with dth (1024 in a wall street journal (WSL) portion to thePenn Treehouse, a high-frequency training sentence. We also trained in a semi-supervised setting.We trained a larger high-confidence and Berkeley Parse corpora with approximately 17M sentencesfor the semi-surveillance token set, for the WSJ with setting and a vocabulary of 32K tokens(Section 1). Learning rates for beam size and the Section 1 development set. Both attention and residualremained unchanged from the\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Table 3: Variations on the Transformer architecture**\n",
      "\n",
      "| Model | Parameters | Per-word Perplexity | BLEU Score |\n",
      "| --- | --- | --- | --- |\n",
      "| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\n",
      "| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\n",
      "| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\n",
      "| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\n",
      "| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\n",
      "\n",
      "**Notes**\n",
      "\n",
      "* All metrics are on the English-to-German translation development set, newsletter2013.\n",
      "* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\n",
      "* We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "* We present results in Table 1.\n",
      "* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\n",
      "* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\n",
      "* This suggests that determining compatibility is not easy and over-sophisticated quality.\n",
      "* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\n",
      "* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\n",
      "\n",
      "**English Constituency Parsing**\n",
      "\n",
      "* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\n",
      "* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\n",
      "* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\n",
      "* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\n",
      "* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.']\n",
      "{'<OCR>': '\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSI 23 PTVinyls & Kaiser al. (2014) (27)WSI only, discriminative98.3Petrov et al., (2006) 29WSI Only, discriminant90.4Zhu et al., (2013) (40WSL only, discriminate94.4Transformer (4.4) (20.6)WSJ only, discriminatory91.3Huawei only. discriminant91.2Huawei no.1 (2013). (21.3)HUAWEI only.supervised91.1Huawei non-supervised92.1Vinayas et al, (2015) (23.7)Semi-supported92.3Vinaysai et al (2015). (24.5) (37)multi-task92.2Lugong et al.(2016) (2017) (2018) (19.1)generative93.3increased the maximum output length in input length + 300. We used a beam size of 21 and a = 0.3for both WSJ only and the semi-suspended setting.Our results in Table 1 show that the lack of task-specific tuning of our model performs surprisingly well, networking better better than the previously reported models with the exception of theRecurrent Neural Network Grammar (8).In RNN sequence-to-sequence models (57). The Transfermer outperforms the Berkeley-Parsers 23 (29) when when the WRSI only on the WSJ set of 40K sentences.7 ConclusionIn this work, we presented the Transformer, the first sequence transition model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transmitter can be trained significantly faster than architectures basedon recurrent or convolutional layers. On both WMT 2014 English-German and WMT 2014model outperforms even if previously based ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks. Weplan to extend the process to problem based input and output other than text andsuch as images, Audio and video. Making generation input sequential is another large goal of ours.The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensorAcknowledgements We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint[2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output based on the OCR text:\n",
      "\n",
      "**Table 4: The Transformer generalizes well to English constituency parsing**\n",
      "\n",
      "| Model | WSJ only | Discriminative |\n",
      "| --- | --- | --- |\n",
      "| Vinyls & Kaiser et al. (2014) | 98.3 | - |\n",
      "| Petrov et al. (2006) | 90.4 | - |\n",
      "| Zhu et al. (2013) | 94.4 | - |\n",
      "| Transformer (4.4) | 91.3 | - |\n",
      "| Huawei only | 91.2 | - |\n",
      "| Huawei no.1 (2013) | 21.3 | - |\n",
      "| Huawei only (supervised) | 91.1 | - |\n",
      "| Huawei non-supervised | 92.1 | - |\n",
      "| Vinayas et al. (2015) | 23.7 | - |\n",
      "| Vinaysai et al. (2015) | 24.5 | - |\n",
      "| Multi-task | 92.2 | - |\n",
      "| Lugong et al. (2016) | 19.1 | - |\n",
      "| Generative | 93.3 | - |\n",
      "\n",
      "**Results**\n",
      "\n",
      "* The Transformer generalizes well to English constituency parsing.\n",
      "* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "* We presented the Transformer, the first sequence transition model based entirely on attention.\n",
      "* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "* The Transformer outperforms even previously based ensembles.\n",
      "\n",
      "**Future Work**\n",
      "\n",
      "* We plan to apply attention-based models to other tasks.\n",
      "* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\n",
      "* Making generation input sequential is another large goal of ours.\n",
      "\n",
      "**Code**\n",
      "\n",
      "* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\n",
      "\n",
      "**Acknowledgements**\n",
      "\n",
      "* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\n",
      "\n",
      "**References**\n",
      "\n",
      "* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\n",
      "* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly']\n",
      "{'<OCR>': '\\n[5] Kyunghyun Cho, Bart van Merrionboer, Caglar Gulechere, Fethi Bougares, Holger Schwenk,and Yoshua Bengo. Learning phrase representations using rn encoder-decoder for statisticalmachine translation. Corr. 4/06/2016, 2014.[6] Francis Chollet. Exception: Learning learning with depthwise separate convolutions. arxivpreparing (arxiv-10/16/2016).[7] Junyung Chung, Deep GÃ¼llehre, Kyunghuen Cho, and Yoshua Bengio. Empirical evaluationof gated recurrent neural networks on sequence modeling. CoRr. ab/14/12/3555, 2014.[8] Chris Dyer, Michael A. Pruce, Miguel Ballestereos, and Noah A. Smith. Current neuralnetwork grammars. In Aoc. of NACL, 2016.[9] Jonas Gehring, Michael Pruce, Denis Yarsas, and Yann A. Dauphin, Convolu-[10] Alex Graves. Generating sequences: anxiv/705/02/2, 2017.[11] Kaiming Xi, Xiangyang Zhang Ren, and Jian Sun. Deep residual learning for in-age recognition. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 7/08/2016.[12] Sepp Hocherter, Yoshua Bengio, Paolo Frasconi, and Urgen Schmidlubber. Gradient flow inrecurrence nets: the difficulty of learning long-term dependencies, 2001.[13] Zhongshuang Huang and Jurgen Schindler, self-training PCFG grammar with related computation.[14] Zhonghuang Hung and Mary Harper, Self-Training PCFG Grammar with Metals in Naturalaccuracy and Processing of the Self-Conference on Empowerment Methods in NaturalLanguage Processing, Vinyals, 832-841, Acl. August 2009.[15] Rafal Jozefozek, Oriol Vinyal, Bizi Cluster, Noam Shazer, and Youghui Wu. Exploringthe limits of language modeling. Can active memory replace attention? In Advances in NeuralInformation Processing System, NIPS, GPUS learn algorithms. In International Conference[17] Lukaska Kaiser and Ilya Sysketer, Karen Simonya, Aaron van Dord, Alex Graves, and Ko-on Kon Kalchberg, Lasse Esposholt, Karen SIMONYa, and Arxiv (70/10/2016). In Neural translation in line time. Aaxiv/700/1000/2.2017.[19] Yoon Kim, Carl Canton Huang, and Alexander R. Rush. Structured\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**References**\n",
      "\n",
      "1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\n",
      "2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\n",
      "3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\n",
      "4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\n",
      "5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\n",
      "6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\n",
      "7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\n",
      "8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\n",
      "9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\n",
      "10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\n",
      "11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\n",
      "12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\n",
      "13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly', 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...']\n",
      "{'<OCR>': '\\n[25] Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. Building a large annotatedcorpus in english: The pen trebank, Computational linguistics, 1992-213-330, 1993.[26] David McCloskey, Eugene Chamik, and Mark Johnson, Effective self-training for parsing. InProceeding the Human Language Technology Conference of the NAACL, Main Conference,pages 152-159, ACL, June 2006.[27] Ankur Parik, Oscar Tickem, Dipanjan Das, and Jakob Uskoroti, a decomposable attentionmodel in Empirical Methods in Natural Language Processing, 2016.[28] Paulus, Caiming Methods in Richard A. Schoeer, A.R. Reinforced model for abstractscummarization in pre-preriin (axm-03-04) and Dan Klein, Learning accurate, compact,and interpretational annotation. In Proceedings of the 21st International Conference onCompetational Logistics and 44th Annual Meeting of the ACL, 433-440, ACL, July2006.[30] Ofir Press and Lior Wolf, Using the output embedding to improve language models. arXiv[31] Noam Semrink, Barry Haddow, and Alexandra Birch, Neural machine translation of rare wordspreparing (axr-08-05-08) 2016.[32) Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Davis, Quoc Le, Geoffrey Hinton,and Jeff Shazer. Outrageously interpret neural networks. The sparsely-gated mixture-of-expertslayer, axr-07-10-06-2017.[33] Sivastava, Geoffrey E. Hilton, Ilya Krizskev, Ilya Suksutzer, and Journal Salahuthi-nov. Dropper, (19-19-09-2014), 2014 networks oververting, over-the-Machine[34] Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson, End-to-end memorynetworks in C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors,Advances in Neural Information Processing Systems, pages 240-248, Curran Associates,Inc., 2015.[35] Ilya SÃ¼kster, Oriol Vinayal, and Quoc V. Le. sequence learning with neuralnetworks. In Advances in Inventive Information Processing System, page 324-312, 2014.[36] Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna.Rethinking in Inception Architecture for computer vision. Jonathan Shelnes, a.l. ab512-\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**References**\n",
      "\n",
      "1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\n",
      "2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\n",
      "3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\n",
      "4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\n",
      "5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\n",
      "6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\n",
      "7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\n",
      "8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\n",
      "9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\n",
      "10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\n",
      "11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly', 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...', 'Here is the structured output:\\n\\n**References**\\n\\n1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\\n2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\\n3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\\n4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\\n5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\\n6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\\n7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\\n8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\\n9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\\n10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\\n11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.']\n",
      "{'<OCR>': '\\n13Attention VisualizationsFigure 3. An example of the attention mechanism following long-distance dependencies in theencoder self-attention in layer 5 of 6. Many of the Attention heads attend to a distant dependency ofthe verb \"making\", completing the phrase making - more difficult. \"Attention\\'s here shown only forthe word \"making\". Different colors represent different heads. Best viewed in color.\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: Here is the structured output:\n",
      "\n",
      "**Attention Visualizations**\n",
      "\n",
      "**Figure 3**\n",
      "\n",
      "**An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.**\n",
      "\n",
      "**Many of the Attention heads attend to a distant dependency of the verb \"making\", completing the phrase making - more difficult.**\n",
      "\n",
      "**\"Attention's here shown only for the word \"making\".**\n",
      "\n",
      "**Different colors represent different heads.**\n",
      "\n",
      "**Best viewed in color.**\n",
      "\n",
      "Let me know if you'd like me to format it in a specific way or if this meets your requirements!\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly', 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...', 'Here is the structured output:\\n\\n**References**\\n\\n1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\\n2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\\n3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\\n4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\\n5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\\n6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\\n7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\\n8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\\n9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\\n10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\\n11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.', 'Here is the structured output:\\n\\n**Attention Visualizations**\\n\\n**Figure 3**\\n\\n**An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.**\\n\\n**Many of the Attention heads attend to a distant dependency of the verb \"making\", completing the phrase making - more difficult.**\\n\\n**\"Attention\\'s here shown only for the word \"making\".**\\n\\n**Different colors represent different heads.**\\n\\n**Best viewed in color.**\\n\\nLet me know if you\\'d like me to format it in a specific way or if this meets your requirements!']\n",
      "{'<OCR>': '\\nTheTheTheLawTheThe LawThe LawWillWillWillwillwillwillWillwillbebebewillwillbewillbeperfectwillwillthewillwillneverwillwillwewillwillnotwillwillwwillwill.willwill willwillwill bebebeperfectbebeapplicationshouldwillbejustjustjustwillwillisjustjustisjustwilliswillwillthiswillisthiswillwillwaswilliswhatwillisiswilliswaswillwillissingwillwillmissingwilliswewillisissingwiswillwissingwewillwisisiswasissinginininmywillwillinmyinmyoptionoptionoptioninmyEOSoptionoptionEOS-EOS - EOS-eOS - eOS -eOS- eOS-espapapapAPAPAPapAPapap-APAP-AP-ap-apAP-applicationapplicATIONapplicatonshouldjustjustusjustjustthisisjustuswillis.willis justjustususjustus justusususwillwillwouldwilliswiswississingwaswaswasweissing-inin-inmy-in-nmyoption-in -in-myoptionopinionoptionoption-eos -eos-eOs -espap-appl\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: The OCR text appears to be a scanned page of text with some formatting and punctuation errors. Here is a structured output with the text reformatted to improve readability:\n",
      "\n",
      "The Law\n",
      "The Law Will Be Perfect\n",
      "Will Will Will Will Will Will Will\n",
      "Will Be Perfect\n",
      "Applications Should Be Just\n",
      "Just Is Just This Is Just\n",
      "This Is What Is What Is Was\n",
      "What Is Missing Is What Is\n",
      "What Is Missing Is What Is\n",
      "What Is Missing Is What Is\n",
      "In My Option\n",
      "Option Option EOS\n",
      "EOS - EOS - eOS - eOS - eOS\n",
      "papapap APAPAP apAP - APAP - AP - ap - AP\n",
      "Application Should Just Us\n",
      "Just Us Just Us Just Us\n",
      "Will Will Would Is Wis\n",
      "Was Was Was We Is\n",
      "In In In My Option\n",
      "In - In - My Option\n",
      "Opinion Option Option EOS\n",
      "eos - eos - eOs - espap - appl\n",
      "\n",
      "Note that I did not add any information that was not present in the original OCR text. I only reformatted the text to improve readability and removed some of the unnecessary whitespace characters.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly', 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...', 'Here is the structured output:\\n\\n**References**\\n\\n1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\\n2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\\n3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\\n4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\\n5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\\n6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\\n7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\\n8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\\n9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\\n10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\\n11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.', 'Here is the structured output:\\n\\n**Attention Visualizations**\\n\\n**Figure 3**\\n\\n**An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.**\\n\\n**Many of the Attention heads attend to a distant dependency of the verb \"making\", completing the phrase making - more difficult.**\\n\\n**\"Attention\\'s here shown only for the word \"making\".**\\n\\n**Different colors represent different heads.**\\n\\n**Best viewed in color.**\\n\\nLet me know if you\\'d like me to format it in a specific way or if this meets your requirements!', 'The OCR text appears to be a scanned page of text with some formatting and punctuation errors. Here is a structured output with the text reformatted to improve readability:\\n\\nThe Law\\nThe Law Will Be Perfect\\nWill Will Will Will Will Will Will\\nWill Be Perfect\\nApplications Should Be Just\\nJust Is Just This Is Just\\nThis Is What Is What Is Was\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nIn My Option\\nOption Option EOS\\nEOS - EOS - eOS - eOS - eOS\\npapapap APAPAP apAP - APAP - AP - ap - AP\\nApplication Should Just Us\\nJust Us Just Us Just Us\\nWill Will Would Is Wis\\nWas Was Was We Is\\nIn In In My Option\\nIn - In - My Option\\nOpinion Option Option EOS\\neos - eos - eOs - espap - appl\\n\\nNote that I did not add any information that was not present in the original OCR text. I only reformatted the text to improve readability and removed some of the unnecessary whitespace characters.']\n",
      "{'<OCR>': '\\nTheTheLawThe LawThe Lawthe LawTheLawthe LawtheLawthe lawthe Lawwillwillwillthe lawwillwillneverneverneverwillneverwillwillbebebeperfectbeperfectperfectbebeperfectbeperfectapplicationshouldbebejustbebeapplicationapplicatiesshouldapplicatiopplicatioshouldbejustjustjustbejustthisthisthisisthisisjustthisisisjustjustthis isthisiswasthisiswhatwaswaswasthiswaswaswhatwaswhatiswaswasissingwasmissingmissinginininmymyinmyoptionoptionoptioninmyopinioninmyecds-ecDs-ecds -ECDS-ECDs-ECDS -ECDs -ecDs -ECOs -ECOS-ECOs-ECOS -ECS -ECD-ECS-ECO-ECSD-ECD -ECSD -ECO -ECOD-ECOD -ECDP -ECDF -ECOF -ECOP -ECOSP -ECSP -ECCP -ECPS-ECDP-ECOP-ECSP-ECCP-ECPS -ECP-ECOSP-ECP -ECGP -ECBP -ECPOS -ECOPS -ECCSP -EGDP -EGCP -EGP -EGPS -EGSP -ECP - ECP-EGP-EGCP-ECP-\\n'}\n",
      "input_variables=['ocr_text'] template='\\n    Given is the OCR text from a full page, please return a structured output with out missing any information. Please do not add any information that is not in the OCR text.\\n    OCR Text:\\n    {ocr_text}\\n    '\n",
      "Structured OCR: The OCR text appears to be a single block of text without any clear structure or formatting. However, I can try to extract some information from it. Here is a structured output based on the OCR text:\n",
      "\n",
      "**Text:**\n",
      "\n",
      "The Law will never be perfect. Applications should be just this. This is what was missing in my option. In my opinion, this is what was missing in my option.\n",
      "\n",
      "**Keywords:**\n",
      "\n",
      "* Law\n",
      "* Perfect\n",
      "* Applications\n",
      "* Option\n",
      "* Opinion\n",
      "\n",
      "**Sentences:**\n",
      "\n",
      "1. The Law will never be perfect.\n",
      "2. Applications should be just this.\n",
      "3. This is what was missing in my option.\n",
      "4. In my opinion, this is what was missing in my option.\n",
      "\n",
      "Please note that the OCR text is quite noisy and contains many errors, which may affect the accuracy of the extracted information.\n",
      "Structured OCR: ['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].', 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.', 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.', 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]', 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |', 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.', 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.', 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.', 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly', 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...', 'Here is the structured output:\\n\\n**References**\\n\\n1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\\n2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\\n3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\\n4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\\n5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\\n6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\\n7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\\n8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\\n9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\\n10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\\n11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.', 'Here is the structured output:\\n\\n**Attention Visualizations**\\n\\n**Figure 3**\\n\\n**An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.**\\n\\n**Many of the Attention heads attend to a distant dependency of the verb \"making\", completing the phrase making - more difficult.**\\n\\n**\"Attention\\'s here shown only for the word \"making\".**\\n\\n**Different colors represent different heads.**\\n\\n**Best viewed in color.**\\n\\nLet me know if you\\'d like me to format it in a specific way or if this meets your requirements!', 'The OCR text appears to be a scanned page of text with some formatting and punctuation errors. Here is a structured output with the text reformatted to improve readability:\\n\\nThe Law\\nThe Law Will Be Perfect\\nWill Will Will Will Will Will Will\\nWill Be Perfect\\nApplications Should Be Just\\nJust Is Just This Is Just\\nThis Is What Is What Is Was\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nIn My Option\\nOption Option EOS\\nEOS - EOS - eOS - eOS - eOS\\npapapap APAPAP apAP - APAP - AP - ap - AP\\nApplication Should Just Us\\nJust Us Just Us Just Us\\nWill Will Would Is Wis\\nWas Was Was We Is\\nIn In In My Option\\nIn - In - My Option\\nOpinion Option Option EOS\\neos - eos - eOs - espap - appl\\n\\nNote that I did not add any information that was not present in the original OCR text. I only reformatted the text to improve readability and removed some of the unnecessary whitespace characters.', 'The OCR text appears to be a single block of text without any clear structure or formatting. However, I can try to extract some information from it. Here is a structured output based on the OCR text:\\n\\n**Text:**\\n\\nThe Law will never be perfect. Applications should be just this. This is what was missing in my option. In my opinion, this is what was missing in my option.\\n\\n**Keywords:**\\n\\n* Law\\n* Perfect\\n* Applications\\n* Option\\n* Opinion\\n\\n**Sentences:**\\n\\n1. The Law will never be perfect.\\n2. Applications should be just this.\\n3. This is what was missing in my option.\\n4. In my opinion, this is what was missing in my option.\\n\\nPlease note that the OCR text is quite noisy and contains many errors, which may affect the accuracy of the extracted information.']\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "structured_ocr_text = []\n",
    "florence_ocr_client = Client(\"gokaygokay/Florence-2\")\n",
    "for page_number, image_file in enumerate(image_files, start=1):\n",
    "    result = florence_ocr_client.predict(\n",
    "    image=handle_file(image_file),\n",
    "    task_prompt=\"OCR\",\n",
    "    text_input=None,\n",
    "    model_id=\"microsoft/Florence-2-large\",\n",
    "    api_name=\"/process_image\"\n",
    "    )\n",
    "    ocr_text = result[0]\n",
    "    print(ocr_text)\n",
    "\n",
    "    structured_text_fromLLM = structure_ocr_with_llm(ocr_text)\n",
    "    structured_ocr_text.append(structured_text_fromLLM)\n",
    "    \n",
    "    print(\"Structured OCR:\", structured_ocr_text)\n",
    "    \n",
    "    # metadata = {\"filename\": filename, \"page_number\": page_number}\n",
    "    # doc = Document(page_content=\"\".join(ocr_text), metadata=metadata)\n",
    "    # documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is the structured output:\\n\\n**Title:** Attention Is All You Need\\n\\n**Authors:**\\n\\n1. Ashish Vaswani\\n2. Noam Shawezer\\n3. Nikki Parmar\\n4. Jakob Uszkoreit\\n5. Lion Jones\\n6. Aidan N. Comer\\n7. Lukasz Kaiser\\n\\n**Affiliations:**\\n\\n1. Google Brain\\n2. Google Research\\n3. University of Toronto\\n\\n**Contact Information:**\\n\\n1. Ashish Vaswani: avaswan@google.com\\n2. Noam Shawezer: noam@Google.com\\n3. Nikki Parmar: nikipogogle.com\\n4. Jakob Uszkoreit: uszgogle.com\\n5. Lion Jones: lion@google.com\\n6. Aidan N. Comer: aidananda, toronto.edu\\n7. Lukasz Kaiser: lukaszkaiser@google.com\\n8. Illia Poloshkin: lila.poloshkin@gmail.com\\n\\n**Abstract:**\\n\\nThe dominant sequence translation models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performance is achieved by a new network architecture, the Transformer mechanism, and the idea. A new network structure, the Transformer mechanism, and its ability to communicate with each other. Based solely on quality mechanisms, dispensing with recurrence and convolutional superiority to evaluate while being more parallelable and requiring significantly less time to train. While other models, the model establishes a single-language state-of-the-art BLEU score of 41.8 after the best models for a literature.\\n\\n**Paper Information:**\\n\\n* Paper ID: arXiv.1706.03760\\n* Category: cs.CLI\\n* Date: 2 Aug 2023\\n\\n**Note:**\\n\\n* Equal contribution\\n* Listing is random\\n* Jakob proposed replacing RNNS with self-transformer models and the effort to evaluate this article.\\n* Ashish, Ashish and Jakob developed and implemented the Transformer models and attention, the parameter-free position tuned and became the other persons involved in nearly every detail.\\n* Niki designed, implemented, and evaluated countless models in our original code and implemented.\\n* Nikis developed, implemented and evaluated the model person involved in nearly every efficient inference and visualizations, Lukas and Aidan spent countless long designing various parts of and our research.\\n* Work performed while at Google Brain.\\n* Presented at the 1st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\nRecurrent neural networks, long-short-term memory (LSTM) and gated recurrent neural networks (GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling. Time-of-recurrent language models and encoder-decoder efforts have since been continued to push the boundaries of current language models in input and output architectures.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output states, which generate a sequence of hidden sequential prerequisites parallelization within training times, which becomes critical at longer computation times. This conditional computation has also been improving model performance in case of the latter.\\n\\n**Background**\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [1], Bythene [18], and Conv [25], all of which use computational neural networks in basic building blocks, complex hidden representations in parallel use and input-output positions. In these models, the number of operations required to relate signals from two arbitrary input positions grows reduced to a certain number of operations, at the cost of reduced Multi-Resolution, which makes it more difficult to learn dependencies between two columns.\\n\\n**Transformer**\\n\\nThe Transformer is a model architecture that eschews recurrence and instead allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Model Architecture**\\n\\nThe Transformer allows for significantly more parallelization to reach a level of transformation quality after being aligned for more little hours on eight P100 GPUs.\\n\\n**Self-Attention**\\n\\nSelf-attention, sometimes called compute, is an attempt mechanism to compute a representation of the sequence by attending to different positions of a single sequence. Self-attention has been used for textual maintenance and learning task-dependent sentence representations [27] and [27] instead of sequence-end-to-end memory networks, which are based on a recent sequence mechanism question answering and aligned recurrence in learning tasks.\\n\\n**Advantages**\\n\\nThe Transformer is the first transition model relying entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs and convolution. In the following sections, we will describe the self-attention mechanism and discuss its advantages over models such as [1] and [2].',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**The Transformer Model**\\n\\n**Model Architecture**\\n\\nThe Transformer model consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of identical layers, with the encoder having 6 layers and the decoder having 3 layers.\\n\\n**Encoder**\\n\\n* The encoder is composed of a stack of 6 identical layers.\\n* Each layer has two sub-layers:\\n\\t1. A multi-decoder self-attenuation mechanism.\\n\\t2. A simple, position-wise fully connected feed-forward network.\\n* Residual connections are employed around each of the two layers.\\n\\n**Decoder**\\n\\n* The decoder is composed of a stack of 3 identical layers.\\n* Each layer performs multi-head self-attentions, followed by layer normalization.\\n* The self-attentions are modified to prevent embeddings by one of the sub-sections, which prevents the predictions for position from depending on the known outputs at positions less than 1.\\n\\n**Attention**\\n\\n* An attention function maps a query and a set of key-value pairs to an output.\\n* The output is computed as a weighted sum of the values, where the weights are computed based on the similarity between the query and the keys.\\n\\n**Additional Details**\\n\\n* The embedding is implemented as well as the function.\\n* The outputs of the sub-layers in the encoder and decoder are of dimension (5:1).\\n* The model uses a residual connection (1) around each of the two layers.\\n\\n**Figure 1**\\n\\nThe Transformer model architecture is shown in Figure 1, with the encoder and decoder layers depicted in the left and right halves of the figure, respectively.',\n",
       " 'Here is the structured output:\\n\\n**Title:** Sealed Dot-Product Attention and Multi-Head Attention\\n\\n**Section 3.2:** Attention Mechanisms\\n\\n**3.2.1 Sealed Dot-Product Attention:**\\n\\n* Input: queries and keys of dimension d, d, and values of dimension da\\n* Compute dot products of queries with all keys, divide each by a scaling factor, and apply a softmax function simultaneously\\n* Formula: Attention(Q, K) = softmax(QK)\\n* The two most commonly used attention functions are additive addition and dot-product multiplicative attention\\n* Dot-product attention is used in our algorithm, except for the scaling factor\\n* Additive attention is more space-efficient in practice, since it can be implemented highly optimized matrix multiplex\\n\\n**3.2.2 Multi-Head Attention:**\\n\\n* Instead of performing a single attention function with d-model-dimensional keys, values, and queries\\n* Linearly identify the queries, keys, and values with different, learned linear projections to de, d/e, and d/f, and dimensions, respectively\\n* Perform attention function in parallel on each projection, yielding d-dimensional output\\n* This approach is beneficial and outperforms single-head attention\\n\\n**Figure 2:** (left) Sealed Dot-Product Attention, (right) Multi-Head Attention\\n\\n**Figure 3:** Scaled Dot-Product Attention\\n\\n**Note:** The text does not provide a specific formula for the additive attention mechanism, only mentioning that it uses a feed-forward network with a scaling factor.',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Multi-Head Attention**\\n\\n* Allows the model to jointly attend to information from different representation subspaces at different positions.\\n* With a single attention head, averaging inhibits this.\\n\\n**Mathematical Representation**\\n\\n* Multi-Head(Q, K, V) = Concat(head1, ..., headJ)\\n* headj = Attention(QWj, K, WVj)\\n* Where:\\n\\t+ Q, K, V are input matrices\\n\\t+ Wj are parameter matrices\\n\\t+ d is the dimensionality of the input\\n\\t+ h is the number of heads\\n\\t+ J is the number of parallel attention layers\\n\\n**Parameter Matrices**\\n\\n* W2 = [R, R, R, E, R, E, E, R]\\n* WK, K, WY, WV are parameter matrices\\n* I/O = (d) = (h) = [h] + [h]\\n\\n**Applications of Attention in our Model**\\n\\n* The Transformer uses multi-attention layers in three different ways:\\n\\t1. Encoder-multi attention layer: uses the previous decoder layer and the memory keys and values from the output in the encoder.\\n\\t2. Decoder-decoder attention mechanism: allows every position in the decoder to attend all positions in the input sequence.\\n\\t3. Self-attention layer: all keys, values, and queries come from the same place (the output of the previous layer in the encoder).\\n\\n**Preventing Leftover Inside Scaled Data-Product Response**\\n\\n* Masking (setting) all values in the software that correspond to illegal connections.\\n\\n**Position to Attention Feed-Forward Networks**\\n\\n* Each layer in the encoder and decoder contains a fully connected two-linear network, which is applied to each position separately and identically.\\n* The network consists of two linear transformations with an applied ACTU activation in between.\\n* The dimensionality of the output and output is (6, 12, ...), and the inner-layer has dimensionality dff = 2014.34.\\n\\n**Embeddings and Softmax**\\n\\n* Similar to [insert reference or description]',\n",
       " 'Here is the structured output:\\n\\n**Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types**\\n\\n| **Layer Type** | **Complexity per Layer** | **Sequential Operations** | **Maximum Path Length** |\\n| --- | --- | --- | --- |\\n| 0 | 1 | 1 | 2 |\\n| 1 | 1 | 2 | 3 |\\n| 2 | 1 | 3 | 4 |\\n| 3 | 1 | 4 | 5 |\\n| 4 | 1 | 5 | 6 |\\n| 5 | 1 | 6 | 7 |\\n| 6 | 1 | 7 | 8 |\\n| 7 | 1 | 8 | 9 |\\n| 8 | 1 | 9 | 10 |\\n| 9 | 1 | 10 | 11 |\\n| 10 | 1 | 11 | 12 |\\n| 11 | 1 | 12 | 13 |\\n| 12 | 1 | 13 | 14 |\\n| 13 | 1 | 14 | 15 |\\n| 14 | 1 | 15 | 16 |\\n| 15 | 1 | 16 | 17 |\\n| 16 | 1 | 17 | 18 |\\n| 17 | 1 | 18 | 19 |\\n| 18 | 1 | 19 | 20 |\\n| 19 | 1 | 20 | 21 |\\n| 20 | 1 | 21 | 22 |\\n| 21 | 1 | 22 | 23 |\\n| 22 | 1 | 23 | 24 |\\n| 23 | 1 | 24 | 25 |\\n| 24 | 1 | 25 | 26 |\\n| 25 | 1 | 26 | 27 |\\n| 26 | 1 | 27 | 28 |\\n| 27 | 1 | 28 | 29 |\\n| 28 | 1 | 29 | 30 |\\n| 29 | 1 | 30 | 31 |\\n| 30 | 1 | 31 | 32 |\\n| 31 | 1 | 32 | 33 |\\n| 32 | 1 | 33 | 34 |\\n| 33 | 1 | 34 | 35 |\\n| 34 | 1 | 35 | 36 |\\n| 35 | 1 | 36 | 37 |\\n| 36 | 1 | 37 | 38 |\\n| 37 | 1 | 38 | 39 |\\n| 38 | 1 | 39 | 40 |\\n| 39 | 1 | 40 | 41 |\\n| 40 | 1 | 41 | 42 |\\n| 41 | 1 | 42 | 43 |\\n| 42 | 1 | 43 | 44 |\\n| 43 | 1 | 44 | 45 |\\n| 44 | 1 | 45 | 46 |\\n| 45 | 1 | 46 | 47 |\\n| 46 | 1 | 47 | 48 |\\n| 47 | 1 | 48 | 50 |\\n| 48 | 1 | 51 | 52 |\\n| 52 | 1 | 53 | 54 |\\n| 54 | 1 | 55 | 56 |\\n| 56 | 1 | 57 | 58 |\\n| 58 | 1 | 59 | 60 |\\n| 60 | 1 | 61 | 62 |\\n| 62 | 1 | 63 | 64 |\\n| 64 | 1 | 65 | 66 |\\n| 66 | 1 | 67 | 68 |\\n| 68 | 1 | 69 | 70 |\\n| 70 | 1 | 71 | 80 |\\n| 80 | 1 | 81 | 82 |\\n| 82 | 1 | 83 | 84 |\\n| 84 | 1 | 85 | 86 |\\n| 86 | 1 | 87 | 88 |\\n| 88 | 1 | 89 | 90 |\\n| 90 | 1 | 91 | 92 |\\n| 92 | 1 | 93 | 94 |\\n| 94 | 1 | 95 | 96 |\\n| 96 | 1 | 97 | 98 |\\n| 98 | 1 | 99 | 100 |\\n| 100 | 1 | 102 | 104 |\\n| 104 | 1 | 105 | 106 |\\n| 106 | 1 | 107 | 108 |\\n| 108 | 1 | 110 | 111 |\\n| 111 | 1 | 112 | 113 |\\n| 113 | 1 | 114 | 115 |\\n| 115 | 1 | 116 | 117 |\\n| 117 | 1 | 118 | 119 |\\n| 119 | 1 | 122 | 123 |\\n| 123 | 1 | 124 | 125 |\\n| 125 | 1 | 126 | 127 |\\n| 127 | 1 | 128 | 129 |\\n| 129 | 1 | 130 | 131 |\\n| 131 | 1 | 132 | 133 |\\n| 133 | 1 | 134 | 135 |\\n| 135 | 1 | 136 | 137 |\\n| 137 | 1 | 138 | 139 |\\n| 139 | 1 | 140 | 145 |\\n| 145 | 1 | 150 | 176 |\\n| 176 | 1 | 170 | 171 |\\n| 171 | 1 | 172 | 173 |\\n| 173 | 1 | 174 | 175 |\\n| 175 | 1 | 176 | 172 |\\n| 172 | 1 | 176 | 187 |\\n| 187 | 1 | 188 | 189 |\\n| 189 | 1 | 190 | 191 |\\n| 191 | 1 | 200 | 220 |\\n| 220 | 1 | 225 | 226 |\\n| 226 | 1 | 227 | 270 |\\n| 270 | 1 | 260 | 267 |\\n| 267 | 1 | 268 | 269 |\\n| 269 | 1 | 208 | 209 |\\n| 209 | 1 | 210 | 211 |\\n| 211 | 1 | 202 | 223 |\\n| 223 | 1 | 204 | 205 |\\n| 205 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 202 |\\n| 202 | 1 | 203 | 204 |\\n| 204 | 1 | 203 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 204 | 205 |\\n| 205 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 208 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 207 | 208 |\\n| 208 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 | 205 | 206 |\\n| 206 | 1 | 208 | 209 |\\n| 209 | 1 | 206 | 207 |\\n| 207 | 1 | 202 | 203 |\\n| 203 | 1 | 204 | 205 |\\n| 205 | 1 | 203 | 204 |\\n| 204 | 1 | 206 | 207 |\\n| 207 | 1 |',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Introduction**\\n\\n* The length of 7 is smaller than the representation dimensionality d, which is often the case with sentence representations used by state-of-the-art models in machine translation.\\n* This is because word-piece (8) and byte-pair (1) representations are used to improve computational performance for tasks involving very long sequences.\\n\\n**Approach**\\n\\n* A single convolutional layer with a kernel width of 8 does not connect two pairs of input and output between any two in the case of diluted convolutionals (18).\\n* Increasing the length of the longest paths or O(0.0%) in two positions in the network.\\n* Convolutional layers are generally more expensive than self-attention layers, but the complexity of a separate convolutional layer is equal to the combination of self-attention and a point-wise feed-forward layer.\\n\\n**Motivation**\\n\\n* Self-attention would yield more interpretable models.\\n* We inspect attention distributions from our models and present and discuss examples in the appendix.\\n* Not only individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n**Training**\\n\\n* **Training Data and Batching**\\n\\t+ Trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\\n\\t+ Sentences are encoded using byte-pair encoding (3), which has a shared source-target vocabulary of about 37,000 tokens for English-French, and split into larger batches.\\n* **Hardware and Schedule**\\n\\t+ Trained on one machine with 8 NVIDIA P100 GPUs.\\n\\t+ Base models were trained for 12 hours, with a total of 100,000 steps.\\n\\t+ Big models were trained for 300,000 steps, with a step time of 1.0 seconds.\\n* **Optimizer**\\n\\t+ Used the Adam optimizer (20) with a learning rate of 0.9, Î²1 = 0.98, and Î²2 = 0.9.\\n\\t+ Varying the learning rate over the course of training, according to the formula: rate = 4.0-5.0 = min-step-num 0.5, step-num = 0,9, num = warmup-steps-1.5.',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to French newsletter/2014 tests at a fraction of the training cost.**\\n\\n| Model | EN-DE BLEU | EN-FR BLEU | Training Cost (FLOPS) |\\n| --- | --- | --- | --- |\\n| Deep-Att + Post-Unk (39) | 24.75 | 39.92 | 19 |\\n| GNMT+ RL (58) | 25.16 | 40.96 | 1018.5 |\\n| Consultant + Post/Unk Ensemble (59) | 26.30 | 40.46 | 1010 |\\n| GNMT + RL (858) | 26.30 | 31.16 | 1010 |\\n| Transformer (base model) | 27.63 | 34.16 | 1011 |\\n| Transformer (base model) | 28.11 | 28.3 | 1019 |\\n\\n**Residual Dropout**\\n\\n* Apply dropout to the output of each sub-layer, before it is added to the sub-layer and normalized.\\n* Apply dropouts of the sum of the embeddings and the positional encodings in both the encoder and decoder stacks.\\n* For the base model, use a rate of Prop = 0.1.\\n\\n**Label Smoothing**\\n\\n* Employ label smoothing of value (2,0) during training.\\n* This hurts perplexity as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n**Results**\\n\\n* **Machine Translation**\\n\\t+ On the WNT 2014 English-German translation task, the big transformer model (Transformer (big)) outperforms the best-previously reported scores by more than 2.0 BLEU, establishing a new state-to-BLEU model of 25.4.\\n\\t+ On the WT 2014 English translation tasks, our big transformer achieves a BLEU score of 41.0.\\n\\t+ On the WNT-2014 English-French translation task, our big transformer achieves a BLEU score of 18.7.\\n\\n**Model Configuration**\\n\\n* The competitive model is the big transformer model.\\n* The base models use a single model obtained by averaging the last 5 checkpoints, which were chosen after experimentation on the development set.\\n* For the base models, a dropout rate of Paro = 0.1 is used instead of 0.3.\\n* We used search for a beam size of 4 and length penalty set to 0.63.\\n* These parameters were chosen from the implementation of the deployment set, and we met the maximum output during experimentation.',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Table 3: Variations on the Transformer architecture**\\n\\n| Model | Parameters | Per-word Perplexity | BLEU Score |\\n| --- | --- | --- | --- |\\n| (A) | 512, 2048, 8646 | 4.1 | 65.0 |\\n| (B) | 512, 2048, 25.4 | 5.0 | 25.4 |\\n| (C) | 256, 2323, 25.5 | 4.8 | 25.2 |\\n| (D) | 1024, 1024, 25.7 | 4.7 | 24.0 |\\n| (E) | 1024, 4096, 30.0 | 4.3 | 23.6 |\\n\\n**Notes**\\n\\n* All metrics are on the English-to-German translation development set, newsletter2013.\\n* Per-word perplexities are per-word space, according to our byte-pair encoding, and should be compared to the base model.\\n* We used beam search as described in the previous section, but no checkpoint averaging.\\n* We present results in Table 1.\\n* In Table B (A), we observe the number of attention heads in the attention key and value dimensions.\\n* In Table B (B), we observe that the setting quality keys off with many single-head attention is 0.9 (BLEU) worse than the setting. Quality keys off (2.2).\\n* This suggests that determining compatibility is not easy and over-sophisticated quality.\\n* Bigger models are better and dropout is very helpful in avoiding fit-fitting.\\n* In (E), we replace our sinusoidal base encoding with learned positional embeddings (D) and observe nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* We performed experiments on English constituency parsing to evaluate if the Transformer can generalize to other tasks.\\n* This task presents specific challenges, including strong structural constraints and significantly longer output than the base model.\\n* We trained a 4-layer transformer with 1024 dimensions on the Wall Street Journal (WSJ) portion of the Penn Treebank (25) and a high-frequency training sentence.\\n* We also trained in a semi-supervised setting using the Berkeley Parse corpora with approximately 17M sentences and a vocabulary of 32K tokens (Section 1).\\n* Learning rates for beam size and the Section 1 development set remained unchanged from the base model.',\n",
       " 'Here is the structured output based on the OCR text:\\n\\n**Table 4: The Transformer generalizes well to English constituency parsing**\\n\\n| Model | WSJ only | Discriminative |\\n| --- | --- | --- |\\n| Vinyls & Kaiser et al. (2014) | 98.3 | - |\\n| Petrov et al. (2006) | 90.4 | - |\\n| Zhu et al. (2013) | 94.4 | - |\\n| Transformer (4.4) | 91.3 | - |\\n| Huawei only | 91.2 | - |\\n| Huawei no.1 (2013) | 21.3 | - |\\n| Huawei only (supervised) | 91.1 | - |\\n| Huawei non-supervised | 92.1 | - |\\n| Vinayas et al. (2015) | 23.7 | - |\\n| Vinaysai et al. (2015) | 24.5 | - |\\n| Multi-task | 92.2 | - |\\n| Lugong et al. (2016) | 19.1 | - |\\n| Generative | 93.3 | - |\\n\\n**Results**\\n\\n* The Transformer generalizes well to English constituency parsing.\\n* The Transformer outperforms the Berkeley-Parsers (23) when using the WSJ only on the WSJ set of 40K sentences.\\n\\n**Conclusion**\\n\\n* We presented the Transformer, the first sequence transition model based entirely on attention.\\n* The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\\n* The Transformer outperforms even previously based ensembles.\\n\\n**Future Work**\\n\\n* We plan to apply attention-based models to other tasks.\\n* We plan to extend the process to problem-based input and output other than text, such as images, Audio, and video.\\n* Making generation input sequential is another large goal of ours.\\n\\n**Code**\\n\\n* The code used to train and evaluate our models is available at https://github.com/censorfor/tensorensor\\n\\n**Acknowledgements**\\n\\n* We are grateful to Nal Kalchbrenner and Stephen Gouws for their fruitful comments, corrections, and inspiration.\\n\\n**References**\\n\\n* [1] Jimmy Lei Ba, Jamie Ryan Kinos, and Geoffrey E Hinton. Layer normalization, arXiv preprint\\n* [2] Dimitriy Baidanha, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly',\n",
       " 'Here is the structured output:\\n\\n**References**\\n\\n1. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Corrected on 4/06/2016.\\n2. Chollet, F. (2016). Exception: Learning learning with depthwise separate convolutions. arXiv preparing (arXiv:1610/16/2016).\\n3. Chung, J., GÃ¼llehre, D., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. Corrected on ab/14/12/3555.\\n4. Dyer, C., Pruce, M. A., Ballesteros, M., & Smith, N. A. (2016). Current neural network grammars. In Aoc. of NACL.\\n5. Gehring, J., Pruce, M., Yasar, D., & Dauphin, Y. A. (n.d.). Convolu-... (arXiv:705/02/2, 2017).\\n6. Graves, A. (2017). Generating sequences: anxiv/705/02/2, 2017.\\n7. Xie, K., Zhang, R., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7/08/2016.\\n8. Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrence nets: the difficulty of learning long-term dependencies.\\n9. Huang, Z., & Schindler, J. (n.d.). Self-training PCFG grammar with related computation.\\n10. Hung, Z., & Harper, M. (2009). Self-Training PCFG Grammar with Metals in Natural accuracy and Processing of the Self-Conference on Empowerment Methods in Natural Language Processing, Vinyals, 832-841, Acl. August.\\n11. Jozefowicz, R., Vinyals, O., Bizi, C., Shazeer, N., & Wu, Y. (n.d.). Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural Information Processing System, NIPS, GPUS learn algorithms. In International Conference...\\n12. Kaiser, L., & Sysko, I. (2016). Neural translation in line time. Aaxiv/700/1000/2.2017.\\n13. Kim, Y., Huang, C. C., & Rush, A. R. (n.d.). Structured...',\n",
       " 'Here is the structured output:\\n\\n**References**\\n\\n1. Mitchell P Marcus, Mary Ann Marcinekiewicz, and Beatrice Santorini. (1993). Building a large annotated corpus in English: The Penn Treebank. Computational Linguistics, 19(2), 213-330.\\n2. David McCloskey, Eugene Chamik, and Mark Johnson. (2006). Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159, ACL.\\n3. Ankur Parikh, Oscar Tackem, Dipanjan Das, and Jakob Uskoroti. (2016). A decomposable attention model. Empirical Methods in Natural Language Processing.\\n4. Paulus, Caiming, Richard A. Schoeer, and Dan Klein. (2006). A reinforced model for abstract summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440, ACL.\\n5. Ofir Press and Lior Wolf. (n.d.). Using the output embedding to improve language models. arXiv.\\n6. Noam Shemirik, Barry Haddow, and Alexandra Birch. (2016). Neural machine translation of rare words. arXiv.\\n7. Noam Shazer, Azriya Miroshkin, Kryzstoff Mariyan, and Quoc Le, Geoffrey Hinton, and Jeff Shazer. (2017). Outrageously interpret neural networks. The sparsely-gated mixture-of-experts layer. arXiv.\\n8. Sivastava, Geoffrey E., Ilya Krizhevsky, Ilya Sutskever, and Journal Salahuthi-nov. (2014). Dropper. arXiv.\\n9. Nishik Subhakkar, Arthur Slawan, Jason D. Weston, and Rob Ferguson. (2015). End-to-end memory networks. In C. Cortes, N. Lawrence, D. L. Lee, and R. Gennett, editors, Advances in Neural Information Processing Systems, pages 240-248, Curran Associates, Inc.\\n10. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014). Sequence learning with neural networks. In Advances in Neural Information Processing Systems, page 324-312.\\n11. Iya Szygedy, Vincent Vanhoute, Coffee Coffee, Jonathan Shelns, and Zbigniew Wojna. (n.d.). Rethinking in Inception Architecture for computer vision.',\n",
       " 'Here is the structured output:\\n\\n**Attention Visualizations**\\n\\n**Figure 3**\\n\\n**An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.**\\n\\n**Many of the Attention heads attend to a distant dependency of the verb \"making\", completing the phrase making - more difficult.**\\n\\n**\"Attention\\'s here shown only for the word \"making\".**\\n\\n**Different colors represent different heads.**\\n\\n**Best viewed in color.**\\n\\nLet me know if you\\'d like me to format it in a specific way or if this meets your requirements!',\n",
       " 'The OCR text appears to be a scanned page of text with some formatting and punctuation errors. Here is a structured output with the text reformatted to improve readability:\\n\\nThe Law\\nThe Law Will Be Perfect\\nWill Will Will Will Will Will Will\\nWill Be Perfect\\nApplications Should Be Just\\nJust Is Just This Is Just\\nThis Is What Is What Is Was\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nWhat Is Missing Is What Is\\nIn My Option\\nOption Option EOS\\nEOS - EOS - eOS - eOS - eOS\\npapapap APAPAP apAP - APAP - AP - ap - AP\\nApplication Should Just Us\\nJust Us Just Us Just Us\\nWill Will Would Is Wis\\nWas Was Was We Is\\nIn In In My Option\\nIn - In - My Option\\nOpinion Option Option EOS\\neos - eos - eOs - espap - appl\\n\\nNote that I did not add any information that was not present in the original OCR text. I only reformatted the text to improve readability and removed some of the unnecessary whitespace characters.',\n",
       " 'The OCR text appears to be a single block of text without any clear structure or formatting. However, I can try to extract some information from it. Here is a structured output based on the OCR text:\\n\\n**Text:**\\n\\nThe Law will never be perfect. Applications should be just this. This is what was missing in my option. In my opinion, this is what was missing in my option.\\n\\n**Keywords:**\\n\\n* Law\\n* Perfect\\n* Applications\\n* Option\\n* Opinion\\n\\n**Sentences:**\\n\\n1. The Law will never be perfect.\\n2. Applications should be just this.\\n3. This is what was missing in my option.\\n4. In my opinion, this is what was missing in my option.\\n\\nPlease note that the OCR text is quite noisy and contains many errors, which may affect the accuracy of the extracted information.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_ocr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, summarize=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize: Bool to summarize or not\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text elements. \\\n",
    "    Give a detailed summary of the text that is well optimized for retrieval. you must not provide something like 'the text describe about or the text is about or Here is a concise summary of the text'. You should be straight to the point with summarization. Please provide a summary of the text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", groq_api_key=\"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\" )\n",
    "    \n",
    "    # model = ChatOllama(model=\"llama\", base_url=\"http://localhost:11434\", temperature=0.0)\n",
    "\n",
    "\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "\n",
    "    return text_summaries\n",
    "\n",
    "\n",
    "# Get text, table summaries\n",
    "text_summaries = generate_text_summaries(structured_ocr_text,  summarize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is a detailed summary of the text optimized for retrieval:\\n\\n**Transformer Architecture**: A new network structure, Transformer, replaces recurrent and convolutional neural networks for sequence translation, achieving state-of-the-art performance.\\n\\n**Key Features**: Transformer mechanism, parallelizable, requires less training time, and eliminates recurrence and convolution.\\n\\n**Achievements**: Establishes a single-language state-of-the-art BLEU score of 41.8, surpassing previous models.\\n\\n**Author Contributions**: Ashish, Jakob, and Ashish developed and implemented Transformer models and attention; Niki designed, implemented, and evaluated models; Lukas and Aidan designed and visualized various parts of the research.\\n\\n**Paper Information**: Paper ID: arXiv.1706.03760, Category: cs.CLI, Date: 2 Aug 2023.\\n\\nThis summary provides a concise and informative overview of the text, highlighting the key points and achievements of the Transformer architecture, while also mentioning the author contributions and paper information.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\nRecurrent neural networks (RNNs) like LSTM and GRU are state-of-the-art in sequence modeling, but time-of-recurrent language models and encoder-decoder efforts aim to improve input and output architectures. RNNs factor computation along input and output states, generating parallelization within training times, but conditional computation improves model performance. Extended Neural GPU, Bythene, and Conv models use computational neural networks, parallel hidden representations, and input-output positions, reducing sequential computation but making it harder to learn dependencies.\\n\\nThe Transformer model eschews recurrence, allowing for more parallelization and reaching transformation quality in less time on multiple GPUs. Self-attention is a mechanism to compute sequence representations by attending to different positions, used for textual maintenance and learning sentence representations. The Transformer relies entirely on self-attention, without RNNs and convolution, and its advantages will be discussed in the following sections.\\n\\nThis summary highlights the key concepts, models, and techniques mentioned in the text, making it easy to retrieve the original text elements.',\n",
       " \"Here is a detailed summary of the text optimized for retrieval:\\n\\nTransformer Model: 6-layer encoder, 3-layer decoder, multi-decoder self-attenuation, position-wise fully connected feed-forward network, residual connections, multi-head self-attentions, layer normalization, attention function, weighted sum of values, embedding, output dimension (5:1), residual connection.\\n\\nThis summary provides a concise and structured overview of the Transformer model's architecture, components, and mechanisms, making it easy to retrieve the original text elements.\",\n",
       " 'Here is a detailed summary of the text:\\n\\nSealed Dot-Product Attention computes dot products of queries with keys, divides by a scaling factor, and applies softmax simultaneously. It uses dot-product attention, except for the scaling factor, and is more space-efficient than additive attention.\\n\\nMulti-Head Attention linearly projects queries, keys, and values to different dimensions and performs attention functions in parallel, yielding a d-dimensional output. This approach outperforms single-head attention.\\n\\nNote: Additive attention uses a feed-forward network with a scaling factor, but the specific formula is not provided.',\n",
       " \"Here is a detailed summary of the text optimized for retrieval:\\n\\nMulti-head attention enables simultaneous attention to different representation subspaces at various positions, whereas single attention heads inhibit this. It's represented mathematically as Concat(head1, ..., headJ), where headj = Attention(QWj, K, WVj). Parameter matrices include W2, WK, K, WY, WV, and I/O = (d) = (h) = [h] + [h].\\n\\nThe Transformer model applies multi-attention layers in three ways: encoder-multi attention, decoder-decoder attention, and self-attention. To prevent leftover inside scaled data-product response, masking is used. Each layer in the encoder and decoder contains a fully connected two-linear network with an ACTU activation, with dimensionality (6, 12, ...) and inner-layer dimensionality dff = 2014.34.\\n\\nEmbeddings and softmax are also used, with a reference or description to be inserted.\",\n",
       " 'I apologize, but the provided text is extremely long and repetitive, making it difficult to summarize. However, I can try to provide a concise summary of the main points:\\n\\n* The text appears to be a table with multiple columns and rows, containing numerical values.\\n* The table seems to be structured around the concept of \"layers\" and \"sequential operations\", with each row representing a specific combination of these two factors.\\n* The values in the table are mostly numerical, with some instances of \"1\" and increasing numbers.\\n* The table is extremely long, with many repeated patterns and sequences.\\n\\nWithout further context or information about the purpose or meaning of the table, it is challenging to provide a more detailed summary. If you could provide more context or clarify what you would like to know about the table, I would be happy to try and assist you further.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\n* Sentence representation dimensionality is often smaller than 7 due to word-piece and byte-pair representations.\\n* Convolutional layers have a kernel width of 8 and are more expensive than self-attention layers, but have equal complexity to self-attention and point-wise feed-forward layers.\\n* Self-attention yields more interpretable models, with attention distributions exhibiting behavior related to syntactic and semantic sentence structure.\\n* Models are trained on the WMT 2014 English-German dataset with byte-pair encoding, using 8 NVIDIA P100 GPUs and the Adam optimizer with a learning rate of 0.9.\\n* Training parameters include 12 hours of training for base models, 100,000 steps, and 300,000 steps for big models, with a step time of 1.0 seconds and varying learning rates.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\n**Transformer Model Outperforms Previous State-of-the-Art Models**\\n\\n* Achieves better BLEU scores on English-to-German and English-to-French newsletter tests at a fraction of the training cost.\\n* Outperforms previous models on WNT 2014 English-German translation task with a BLEU score of 25.4.\\n* Achieves BLEU scores of 41.0 on WT 2014 English translation tasks and 18.7 on WNT-2014 English-French translation task.\\n\\n**Model Configuration**\\n\\n* Competitive model is the big transformer model.\\n* Base models use a single model obtained by averaging last 5 checkpoints.\\n* Dropout rate of 0.1 used instead of 0.3 for base models.\\n* Beam size of 4 and length penalty of 0.63 used during experimentation.\\n\\n**Training Techniques**\\n\\n* Residual dropout applied to output of each sub-layer and sum of embeddings and positional encodings.\\n* Label smoothing of value 0.2 used during training.\\n\\nThis summary provides a concise and detailed overview of the text, highlighting the key points and technical details that can be used for retrieval.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\n**Transformer Variations**\\n\\n* Table 3 compares 5 Transformer models with varying parameters, perplexity, and BLEU scores on English-to-German translation.\\n* Models (A) and (D) achieve better results than (B), (C), and (E) in terms of perplexity and BLEU score.\\n* Model (E) uses learned positional embeddings, resulting in nearly identical results to the base model.\\n\\n**English Constituency Parsing**\\n\\n* The Transformer was evaluated on English constituency parsing, a task with strong structural constraints and longer output.\\n* A 4-layer Transformer with 1024 dimensions was trained on the Wall Street Journal portion of the Penn Treebank and the Berkeley Parse corpora.\\n* The model achieved good results in both supervised and semi-supervised settings.\\n\\n**Key Findings**\\n\\n* Bigger models are better, and dropout is helpful in avoiding overfitting.\\n* Determining compatibility is not easy and over-sophisticated quality may not be necessary.\\n* The Transformer can generalize to other tasks, such as English constituency parsing.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\nTransformer generalizes well to English constituency parsing, outperforming Berkeley-Parsers on WSJ set. Trained significantly faster than recurrent or convolutional layer-based architectures. Outperforms previously based ensembles. Future work includes applying attention-based models to other tasks, extending to problem-based input/output, and generating sequential input.',\n",
       " 'Here is a detailed summary of the text, optimized for retrieval:\\n\\n**Machine Learning and Natural Language Processing**\\n\\n* Research papers on machine learning and natural language processing, including neural networks, recurrent neural networks, and statistical machine translation.\\n* Topics include phrase representations, sequence modeling, grammar, and language modeling.\\n* Papers explore techniques such as encoder-decoders, gated recurrent neural networks, and residual learning.\\n* Applications include image recognition, neural translation, and self-training PCFG grammar.\\n\\nThis summary is designed to provide a concise and accurate overview of the text, highlighting the main topics and themes. It should enable efficient retrieval of the original text elements.',\n",
       " 'Here is a detailed summary of the text, optimized for retrieval:\\n\\n**Natural Language Processing and Machine Learning**\\n\\n* Penn Treebank (1993) - large annotated corpus in English\\n* Effective self-training for parsing (2006) - parsing techniques\\n* Decomposable attention model (2016) - attention mechanism in NLP\\n* Reinforced model for abstract summarization (2006) - summarization techniques\\n* Output embedding to improve language models (n.d.) - language model improvements\\n* Neural machine translation of rare words (2016) - machine translation techniques\\n* Outrageously interpret neural networks (2017) - neural network interpretation\\n* Dropper (2014) - neural network regularization\\n* End-to-end memory networks (2015) - memory-based neural networks\\n* Sequence learning with neural networks (2014) - sequence learning techniques\\n* Rethinking Inception Architecture for computer vision (n.d.) - computer vision architecture improvements\\n\\nThis summary provides a concise overview of the text, highlighting the main topics and concepts related to natural language processing, machine learning, and computer vision. The summary is designed to facilitate efficient retrieval of the original text elements.',\n",
       " 'Here is a detailed summary of the text optimized for retrieval:\\n\\n\"Attention mechanism in layer 5 of 6, visualized in Figure 3, highlights long-distance dependencies in encoder self-attention, with many heads attending to distant verb dependencies, including \\'making\\'.\"\\n\\nThis summary is concise, yet informative, and directly addresses the main topic of the text, making it easy to retrieve the original text elements.',\n",
       " 'Here is a detailed summary of the text:\\n\\nThe text appears to be a scanned page with formatting and punctuation errors. It contains repeated phrases and words, including \"Will\", \"Be Perfect\", \"Just\", \"What Is\", and \"Option\". The text also includes various abbreviations and symbols, such as \"EOS\", \"AP\", and \"papapap\". The content is unclear and lacks coherence, with phrases and sentences seemingly disconnected from each other.',\n",
       " \"Here is a detailed summary of the text optimized for retrieval:\\n\\nLaw imperfection, applications, option, opinion.\\n\\nThis summary captures the main themes and concepts from the text, including the idea that the law is imperfect, the importance of applications being a certain way, and the speaker's opinion on what was missing in their option. The summary is concise and focused on the key points, making it easy to retrieve the original text.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tables like images from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then get summary from openbmb for that table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
